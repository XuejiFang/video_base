{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Wavelet Transform in Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import decord\n",
    "from einops import rearrange\n",
    "from diffusers.utils import export_to_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 16, 480, 720]),\n",
       " tensor(-1., dtype=torch.float16),\n",
       " tensor(1., dtype=torch.float16))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = decord.VideoReader('0003.mp4').get_batch(range(16)).asnumpy()\n",
    "video = torch.tensor(video).unsqueeze_(0).permute(0,4,1,2,3).sub(127.5).div(127.5).to(torch.float16)\n",
    "video.shape, video.min(), video.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B, C, T, H, W = 2, 16, 8, 32, 32\n",
    "# tensor = torch.randn(B, C, T, H, W, dtype=torch.float16)\n",
    "B, C, T, H, W = video.shape\n",
    "tensor = video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveletTransform(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WaveletTransform, self).__init__()\n",
    "\n",
    "        h0 = (1 + np.sqrt(3)) / 4\n",
    "        h1 = (3 + np.sqrt(3)) / 4\n",
    "        self.low_pass_filter = torch.tensor([h0, h1], dtype=torch.float16).unsqueeze(0).unsqueeze(0) # [out_channels, in_channels, kernel_height, kernel_width]\n",
    "        g0 = (3 - np.sqrt(3)) / 4\n",
    "        g1 = (1 - np.sqrt(3)) / 4\n",
    "        self.high_pass_filter = torch.tensor([g0, g1], dtype=torch.float16).unsqueeze(0).unsqueeze(0) # [out_channels, in_channels, kernel_height, kernel_width]\n",
    "    def forward(self, x):\n",
    "        low = F.conv1d(x, self.low_pass_filter, stride=2, padding=0)\n",
    "        high = F.conv1d(x, self.high_pass_filter, stride=2, padding=0)\n",
    "        return low, high\n",
    "    def inverse(self, low, high):\n",
    "        x = F.conv_transpose1d(low, self.low_pass_filter, stride=2, padding=0)\n",
    "        x += F.conv_transpose1d(high, self.high_pass_filter, stride=2, padding=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = WaveletTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on h-dim\n",
    "tensor = video\n",
    "tensor = rearrange(tensor, 'b c t h w -> (b c t w) 1 h')\n",
    "h_a, h_d = transform(tensor)\n",
    "h_a = rearrange(h_a, '(b c t w) 1 h -> b c t h w', b=B, c=C, t=T, h=H//2, w=W)\n",
    "h_d = rearrange(h_d, '(b c t w) 1 h -> b c t h w', b=B, c=C, t=T, h=H//2, w=W)\n",
    "# on w-dim\n",
    "h_a = rearrange(h_a, 'b c t h w -> (b c t h) 1 w')\n",
    "h_a_w_a, h_a_w_d = transform(h_a)\n",
    "h_a_w_a = rearrange(h_a_w_a, '(b c t h) 1 w -> b c t h w', b=B, c=C, t=T, h=H//2, w=W//2)\n",
    "h_a_w_d = rearrange(h_a_w_d, '(b c t h) 1 w -> b c t h w', b=B, c=C, t=T, h=H//2, w=W//2)\n",
    "# on t-dim\n",
    "h_a_w_a = rearrange(h_a_w_a, 'b c t h w -> (b c h w) 1 t')\n",
    "h_a_w_a_t_a, h_a_w_a_t_d = transform(h_a_w_a)\n",
    "h_a_w_a_t_a = rearrange(h_a_w_a_t_a, '(b c h w) 1 t -> b c t h w', b=B, c=C, t=T//2, h=H//2, w=W//2)\n",
    "h_a_w_a_t_d = rearrange(h_a_w_a_t_d, '(b c h w) 1 t -> b c t h w', b=B, c=C, t=T//2, h=H//2, w=W//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 8, 240, 360]),\n",
       " tensor(-1.5859, dtype=torch.float16),\n",
       " tensor(1.4922, dtype=torch.float16))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_a_w_a_t_d.shape, h_a_w_a_t_d.min(), h_a_w_a_t_d.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_cthw(tensor):\n",
    "    tensor = rearrange(tensor, 'c t h w -> t h w c')\n",
    "    tensor = ((tensor - tensor.min()) / (tensor.max() - tensor.min())).mul(255).to(torch.uint8).numpy()\n",
    "    video = [Image.fromarray(frame) for frame in tensor]\n",
    "    return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (360, 240) to (368, 240) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "[swscaler @ 0x63f6b40] Warning: data is not aligned! This can lead to a speed loss\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (360, 240) to (368, 240) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "[swscaler @ 0x6089b40] Warning: data is not aligned! This can lead to a speed loss\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (360, 240) to (368, 240) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "[swscaler @ 0x653ca40] Warning: data is not aligned! This can lead to a speed loss\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./h_a_w_d.mp4'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_to_video(view_cthw(h_a_w_a_t_d[0]), './h_a_w_a_t_d.mp4', fps=4)  # 时间上高频，空间上低频 --> motion\n",
    "export_to_video(view_cthw(h_a_w_a_t_a[0]), './h_a_w_a_t_a.mp4', fps=4)  \n",
    "export_to_video(view_cthw(h_a_w_d[0]), './h_a_w_d.mp4', fps=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VidGen-1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_file = '/backup/data/qiguojunLab/VidGen-1M-meta/VidGen_1M_video_caption.json'\n",
    "with open(meta_file, 'r') as f:\n",
    "    meta = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vid': 'Eep9uvenxAo-Scene-0030',\n",
       " 'caption': \"The video shows a person's hand touching and moving flowers on a plant. The flowers are red in color and the plant has green leaves. The person's hand is visible in the foreground, and the background shows a house and a driveway. The video is shot during the daytime, and the lighting is natural.\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opensoraplan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
