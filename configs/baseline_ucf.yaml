vae:
    module: cogvideo.models
    class_name: AutoencoderKLCogVideoX
    params:
        pretrained: /storage/qiguojunLab/qiguojun/home/Models/THUDM/CogVideoX-2b/vae
tokenizer:
    module: transformers
    class_name: T5Tokenizer
    params:
        pretrained: /storage/qiguojunLab/qiguojun/home/Models/google/mt5-base/
text_encoder:
    module: transformers
    class_name: T5EncoderModel
    params:
        pretrained: /storage/qiguojunLab/qiguojun/home/Models/google/mt5-base/
transformer:
    module: opensora.models
    class_name: OpenSoraT2V
    params:
        activation_fn:                  "gelu-approximate"
        attention_bias:                 true
        attention_head_dim:             96
        attention_mode:                 "xformers"
        attention_type:                 "default"
        caption_channels:               768
        cross_attention_dim:            1344
        double_self_attention:          false
        downsampler:                    null
        dropout:                        0.0
        in_channels:                    16
        interpolation_scale_h:          1.0
        interpolation_scale_t:          1.0
        interpolation_scale_w:          1.0
        norm_elementwise_affine:        false
        norm_eps:                       0.000001
        norm_num_groups:                32
        norm_type:                      "ada_norm_single"
        num_attention_heads:            14
        num_embeds_ada_norm:            1000
        num_layers:                     22
        num_vector_embeds:              null
        only_cross_attention:           false
        out_channels:                   16
        patch_size:                     2
        patch_size_t:                   1
        sample_size:                    [32, 32]
        sample_size_t:                  1
        upcast_attention:               false
        use_additional_conditions:      null
        use_linear_projection:          false
        use_rope:                       true
        use_stable_fp32:                false
scheduler:
    module: diffusers
    class_name: FlowMatchEulerDiscreteScheduler
    params:
        shift:                              1.0
pipeline:
    module: opensora.pipelines
    class_name: OpenSoraPipeline

loss_function:
    module: training.losses
    class_name: OpenSoraFMLoss
    params: {}
    
optimizer:
    module: torch.optim
    class_name: AdamW
    params:
        lr:                         0.0002
        betas:                      [0.9, 0.999]
        weight_decay:               0.01
        eps:                        0.00000001
lr_scheduler:
    name:                           cosine
    lr_warmup_steps:                1000

dataloader:
    module: training.dataset
    class_name: VideoDataLoader
    params:
        cfg:                        0.1
        max_text_seq_length:        226
        # vid_pkl_path:               /storage/qiguojunLab/fangxueji/Projects/video_base/dataset/phase3_t2v_v2_webvid_hd_only/webvid-6.6m.pkl
        # vid_pkl_path:               /storage/qiguojunLab/fangxueji/Projects/video_base/dataset/phase4_t2v_v1_hd_finetune/openvid-1m.pkl
        vid_pkl_path:               datasets/ucf-101/train.pkl
        vid_folder:                 '/'
        target_path_keys:           path
        target_prompt_key:          caption
        max_height:                 256
        max_width:                  256
        num_frames:                 29
        sample_rate:                4
        batch_size:                 64
        num_workers:                16
        
training_args:
    team_name:                      maple-video
    exp_name:                       baseline_ucf
    output_dir:                     ./outputs/
    logging_dir:                    logs
    grad_acc:                       1
    mixed_precision:                bf16
    report_to:                      wandb
    ema_decay:                      0.999
    ema_start_step:                 0
    max_train_steps:                50_000
    resume_from_checkpoint:         null
    resume_model:                   outputs/baseline_ucf/checkpoint-0/model/diffusion_pytorch_model.safetensors
    resume_ema_model:               outputs/baseline_ucf/checkpoint-0/model_ema/diffusion_pytorch_model.safetensors
    checkpointing_steps:            500
    checkpoints_total_limit:        100
    gradient_checkpointing:         true
    max_grad_norm:                  1.0
