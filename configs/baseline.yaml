vae:
    module: cogvideo.models.autoencoders
    class_name: AutoencoderKL
    params:
        pretrained: /storage/qiguojunLab/fangxueji/Models/sdxl-vae
tokenizer:
    module: transformers
    class_name: T5Tokenizer
    params:
        pretrained: /storage/qiguojunLab/qiguojun/home/Models/google/mt5-base/
text_encoder:
    module: transformers
    class_name: T5EncoderModel
    params:
        pretrained: /storage/qiguojunLab/qiguojun/home/Models/google/mt5-base/
transformer:
    module: opensora.models
    class_name: OpenSoraT2V
    params:
        activation_fn:                  "gelu-approximate"
        attention_bias:                 true
        attention_head_dim:             96
        attention_mode:                 "xformers"
        attention_type:                 "default"
        caption_channels:               768
        cross_attention_dim:            1344
        double_self_attention:          false
        downsampler:                    null
        dropout:                        0.0
        in_channels:                    4
        interpolation_scale_h:          1.0
        interpolation_scale_t:          1.0
        interpolation_scale_w:          1.0
        norm_elementwise_affine:        false
        norm_eps:                       0.000001
        norm_num_groups:                32
        norm_type:                      "ada_norm_single"
        num_attention_heads:            14
        num_embeds_ada_norm:            1000
        num_layers:                     22
        num_vector_embeds:              null
        only_cross_attention:           false
        out_channels:                   4
        patch_size:                     2
        patch_size_t:                   1
        sample_size:                    [32, 32]
        sample_size_t:                  1
        upcast_attention:               false
        use_additional_conditions:      null
        use_linear_projection:          false
        use_rope:                       true
        use_stable_fp32:                false


loss_function:
    module: training.losses
    class_name: OpenSoraFMLoss
    params: {}
    
optimizer:
    module: torch.optim
    class_name: AdamW
    params:
        lr:                         0.0002
        betas:                      [0.9, 0.999]
        weight_decay:               0.01
        eps:                        0.00000001
lr_scheduler:
    name:                           cosine
    lr_warmup_steps:                1000

dataloader:
    module: training.dataset
    class_name: MdsDataLoader
    params:
        batch_size:                 192
        num_workers:                16
        image_folder:               null
        local_train_dir:            /storage/qiguojunLab/qiguojun/home/Datasets/imagenet.int8/
        cfg:                        0.1
        model_max_length:           512
        
training_args:
    team_name:                      maple-video
    exp_name:                       baseline
    output_dir:                     ./outputs/
    logging_dir:                    logs
    grad_acc:                       1
    mixed_precision:                bf16
    report_to:                      wandb
    ema_decay:                      0.999
    ema_start_step:                 0
    max_train_steps:                200_000
    resume_from_checkpoint:         latest
    checkpointing_steps:            200
    checkpoints_total_limit:        100
    gradient_checkpointing:         true
    max_grad_norm:                  1.0
