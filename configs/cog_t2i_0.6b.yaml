vae:
    module: cogvideo.models
    class_name: AutoencoderKLCogVideoX
    params:
        pretrained: /storage/qiguojunLab/qiguojun/home/Models/THUDM/CogVideoX-2b/vae
tokenizer:
    module: transformers
    class_name: T5Tokenizer
    params:
        pretrained: /storage/qiguojunLab/qiguojun/home/Models/google/t5-v1_1-xxl/
text_encoder:
    module: transformers
    class_name: T5EncoderModel
    params:
        pretrained: /storage/qiguojunLab/qiguojun/home/Models/google/t5-v1_1-xxl/
transformer:
    module: cogvideo.models
    class_name: CogVideoXTransformer3DModel
    params:
        activation_fn:                      gelu-approximate
        attention_bias:                     true
        attention_head_dim:                 64
        dropout:                            0.0
        flip_sin_to_cos:                    true
        freq_shift:                         0
        in_channels:                        16
        max_text_seq_length:                226
        norm_elementwise_affine:            true
        norm_eps:                           0.00001
        num_attention_heads:                18  # 30
        num_layers:                         26  # 30
        out_channels:                       16
        patch_size:                         2
        sample_frames:                      1
        sample_height:                      32
        sample_width:                       32
        spatial_interpolation_scale:        1.875
        temporal_compression_ratio:         4
        temporal_interpolation_scale:       1.0
        text_embed_dim:                     4096
        time_embed_dim:                     512
        timestep_activation_fn:             silu
        use_rotary_positional_embeddings:   false

loss_function:
    module: training.losses
    class_name: CogFMLoss
    params: {}
    
optimizer:
    module: torch.optim
    class_name: Adam
    params:
        lr:                         0.0001
        betas:                      [0.9, 0.999]
        weight_decay:               0.01
        eps:                        0.00000001
lr_scheduler:
    name:                           constant_with_warmup
    lr_warmup_steps:                1000

dataloader:
    module: training.dataset
    class_name: ImageDataLoader
    params:
        batch_size:                 96
        img_folder:                 /
        img_pkl_path:               /storage/qiguojunLab/fangxueji/Projects/video_base/dataset/phase3_t2v_v4.3_joint_youtube_image/image-9m.pkl
        max_height:                 256
        max_width:                  256
        target_prompt_key:          caption
        target_path_keys:           path
        cfg:                        0.1
        num_workers:                16
        max_text_seq_length:        226
        
training_args:
    team_name:                      maple-video
    exp_name:                       cog_t2i_0.6b
    output_dir:                     ./outputs/
    logging_dir:                    logs
    grad_acc:                       1
    mixed_precision:                bf16
    report_to:                      wandb
    ema_decay:                      0.9999
    ema_start_step:                 0
    max_train_steps:                200_000
    resume_from_checkpoint:         latest
    checkpointing_steps:            1000
    checkpoints_total_limit:        100
    gradient_checkpointing:         false
    max_grad_norm:                  10.0
